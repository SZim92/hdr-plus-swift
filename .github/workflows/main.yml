name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: 'Run the build with debug logging'
        type: boolean
        required: false
        default: false

# Prevent multiple CI runs on the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  # Single preflight job for quick validation and change detection
  preflight:
    name: Preflight Check
    runs-on: ubuntu-latest
    outputs:
      should_run_tests: ${{ steps.analyze_changes.outputs.should_run_tests }}
      should_run_metal: ${{ steps.analyze_changes.outputs.should_run_metal }}
      matrix_config: ${{ steps.set_matrix.outputs.matrix_config }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 50
      
      - name: Analyze changes
        id: analyze_changes
        uses: actions/github-script@v7
        with:
          script: |
            const { execSync } = require('child_process');
            
            // Default to true for pushes to main
            let shouldRunTests = true;
            let shouldRunMetal = false;
            
            if (context.eventName === 'pull_request') {
              // For PRs, check what changed
              const baseSha = execSync(`git merge-base origin/${context.payload.pull_request.base.ref} HEAD`).toString().trim();
              const changedFiles = execSync(`git diff --name-only ${baseSha}..HEAD`).toString().split('\n');
              
              // Only skip tests for docs-only changes
              const hasCodeChanges = changedFiles.some(file => 
                file.endsWith('.swift') || 
                file.endsWith('.metal') || 
                file.endsWith('.h') || 
                file.endsWith('.m') || 
                file.endsWith('.cpp') || 
                file.endsWith('.c')
              );
              
              const hasDocChanges = changedFiles.some(file => 
                file.endsWith('.md') || 
                file.includes('/docs/')
              );
              
              const hasMetalChanges = changedFiles.some(file => 
                file.endsWith('.metal') || 
                file.includes('Metal')
              );
              
              shouldRunTests = hasCodeChanges || !hasDocChanges;
              shouldRunMetal = hasMetalChanges;
            }
            
            console.log(`Should run tests: ${shouldRunTests}`);
            console.log(`Should run Metal tests: ${shouldRunMetal}`);
            
            core.setOutput('should_run_tests', shouldRunTests.toString());
            core.setOutput('should_run_metal', shouldRunMetal.toString());
      
      - name: Set test matrix
        id: set_matrix
        run: |
          # Define the test matrix based on what needs to be tested
          if [[ "${{ steps.analyze_changes.outputs.should_run_tests }}" == "true" ]]; then
            echo "matrix_config={\"test_type\":[\"unit\",\"integration\",\"visual\",\"performance\"],\"os\":[\"macos-latest\"]}" >> $GITHUB_OUTPUT
          else
            echo "matrix_config={\"test_type\":[\"unit\"],\"os\":[\"macos-latest\"]}" >> $GITHUB_OUTPUT
          fi
      
      - name: Run SwiftLint
        run: |
          # SwiftLint runs quickly, so we'll include it in preflight
          docker pull ghcr.io/realm/swiftlint:latest
          docker run --rm -v ${{ github.workspace }}:/workspace ghcr.io/realm/swiftlint:latest swiftlint --reporter github-actions-logging
        continue-on-error: true

  # Combined test job using matrix strategy
  tests:
    name: ${{ matrix.test_type }} Tests on ${{ matrix.os }}
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true'
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.preflight.outputs.matrix_config) }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Swift
        uses: swift-actions/setup-swift@v1
        with:
          swift-version: "5.9"
      
      - name: Cache Swift packages and build artifacts
        uses: actions/cache@v4
        with:
          path: |
            .build
            ~/Library/Developer/Xcode/DerivedData/**/SourcePackages/checkouts
            ~/Library/Caches/org.swift.swiftpm
            ~/.swiftpm
          key: ${{ runner.os }}-${{ matrix.test_type }}-swift-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.test_type }}-swift-
      
      - name: Set up test environment
        run: |
          mkdir -p TestResults/${{ matrix.test_type }}
          
          # Set up environment variables for tests
          echo "VERBOSE_TESTING=1" >> $GITHUB_ENV
          echo "TEST_RESULTS_DIR=$(pwd)/TestResults" >> $GITHUB_ENV
          
          # Skip Metal tests if Metal is not available or not needed
          if [[ "${{ matrix.test_type }}" == "metal" && "${{ needs.preflight.outputs.should_run_metal }}" != "true" ]]; then
            echo "SKIP_METAL_TESTS=1" >> $GITHUB_ENV
          fi
          
          # Skip resource-intensive tests in CI unless explicitly testing performance
          if [[ "${{ matrix.test_type }}" != "performance" ]]; then
            echo "SKIP_RESOURCE_INTENSIVE=1" >> $GITHUB_ENV
          fi
      
      - name: Run tests
        id: run_tests
        run: |
          # Determine which test script to use
          if [[ -f "Scripts/run-tests.sh" ]]; then
            chmod +x Scripts/run-tests.sh
            
            # Run the appropriate test type with the standardized test script
            ./Scripts/run-tests.sh \
              --${{ matrix.test_type }} \
              --verbose \
              --html-report \
              --junit-report \
              --results-path TestResults/${{ matrix.test_type }} \
              --retry 2 \
              ${{ github.event.inputs.debug_enabled == 'true' && '--debug' || '' }}
            
            TEST_EXIT_CODE=$?
          else
            # Fallback if script doesn't exist (uses xcodebuild directly)
            echo "Test script not found, using xcodebuild directly"
            
            xcodebuild test \
              -scheme HDRPlusTests \
              -only-testing:${{ matrix.test_type == 'unit' && 'UnitTests' || matrix.test_type == 'integration' && 'IntegrationTests' || matrix.test_type == 'visual' && 'VisualTests' || matrix.test_type == 'performance' && 'PerformanceTests' || matrix.test_type == 'metal' && 'MetalTests' || 'UnitTests' }} \
              -destination "platform=macOS" \
              -resultBundlePath TestResults/${{ matrix.test_type }}/Results.xcresult \
              | tee TestResults/${{ matrix.test_type }}/output.log
            
            TEST_EXIT_CODE=${PIPESTATUS[0]}
            
            # Generate JUnit report
            mkdir -p TestResults/${{ matrix.test_type }}/reports
            xcrun xcresulttool get --format xml --path TestResults/${{ matrix.test_type }}/Results.xcresult > TestResults/${{ matrix.test_type }}/reports/results.xml
          fi
          
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
        
      - name: Process test results
        if: always()
        run: |
          # Extract summary info from test results
          TOTAL=$(grep -c "Test Case.*started" TestResults/${{ matrix.test_type }}/output.log 2>/dev/null || echo 0)
          PASSED=$(grep -c "Test Case.*passed" TestResults/${{ matrix.test_type }}/output.log 2>/dev/null || echo 0)
          FAILED=$(grep -c "Test Case.*failed" TestResults/${{ matrix.test_type }}/output.log 2>/dev/null || echo 0)
          SKIPPED=$((TOTAL - PASSED - FAILED))
          
          echo "## ${{ matrix.test_type }} Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: $PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: $FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped**: $SKIPPED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Include performance results if available
          if [[ "${{ matrix.test_type }}" == "performance" ]] && [[ -f "TestResults/performance/perf_summary.md" ]]; then
            cat TestResults/performance/perf_summary.md >> $GITHUB_STEP_SUMMARY
          fi
          
          # If tests failed, add details
          if [[ "${{ steps.run_tests.outputs.exit_code }}" != "0" ]]; then
            echo "### Failed Tests" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
            grep -A 3 "Test Case.*failed" TestResults/${{ matrix.test_type }}/output.log 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "No detailed failure information available" >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test_type }}-${{ matrix.os }}
          path: TestResults/${{ matrix.test_type }}
          retention-days: 14
      
      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            TestResults/${{ matrix.test_type }}/reports/junit.xml
            TestResults/${{ matrix.test_type }}/reports/results.xml
          check_name: ${{ matrix.test_type }} Test Results
          comment_mode: create
          report_individual_runs: true
          compare_to_earlier_commit: true
          test_changes_limit: 10
      
      - name: Check test outcome
        if: ${{ steps.run_tests.outputs.exit_code != '0' }}
        run: |
          echo "::error::${{ matrix.test_type }} tests failed with exit code ${{ steps.run_tests.outputs.exit_code }}"
          exit 1

  # Summary job to post a consolidated report
  summary:
    name: Test Summary
    needs: [preflight, tests]
    if: always() && needs.preflight.outputs.should_run_tests == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: all-results
          merge-multiple: true
      
      - name: Generate summary report
        if: always()
        run: |
          echo "# HDR+ Swift Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Tests completed at $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Total up the results
          TOTAL_TESTS=0
          TOTAL_PASSED=0
          TOTAL_FAILED=0
          TOTAL_SKIPPED=0
          
          echo "| Test Type | Status | Total | Passed | Failed | Skipped |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|-------|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          for TEST_TYPE in unit integration visual performance metal; do
            if [ -d "all-results/$TEST_TYPE" ]; then
              TESTS=$(grep -c "Test Case.*started" all-results/$TEST_TYPE/output.log 2>/dev/null || echo 0)
              PASSED=$(grep -c "Test Case.*passed" all-results/$TEST_TYPE/output.log 2>/dev/null || echo 0)
              FAILED=$(grep -c "Test Case.*failed" all-results/$TEST_TYPE/output.log 2>/dev/null || echo 0)
              SKIPPED=$((TESTS - PASSED - FAILED))
              
              TOTAL_TESTS=$((TOTAL_TESTS + TESTS))
              TOTAL_PASSED=$((TOTAL_PASSED + PASSED))
              TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + SKIPPED))
              
              if [ $FAILED -gt 0 ]; then
                STATUS="❌ Failing"
              elif [ $TESTS -eq 0 ]; then
                STATUS="⚠️ No tests"
              else
                STATUS="✅ Passing"
              fi
              
              echo "| ${TEST_TYPE^} | $STATUS | $TESTS | $PASSED | $FAILED | $SKIPPED |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| ${TEST_TYPE^} | ⏭️ Skipped | - | - | - | - |" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Overall Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: $TOTAL_PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: $TOTAL_FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped**: $TOTAL_SKIPPED" >> $GITHUB_STEP_SUMMARY
      
      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync(process.env.GITHUB_STEP_SUMMARY, 'utf8');
            
            const statusLines = summary.split('\n').filter(line => line.includes('| ') && !line.includes('---'));
            const statusTable = statusLines.join('\n');
            
            const body = `## Test Results Summary
            
            ${statusTable}
            
            [View detailed test results in the Actions tab](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            }); 