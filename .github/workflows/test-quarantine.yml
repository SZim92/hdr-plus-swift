name: Test Stability Analysis

on:
  schedule:
    - cron: '0 0 * * 0'  # Run weekly at midnight on Sunday
  workflow_dispatch:  # Allow manual triggering
    inputs:
      debug_enabled:
        description: 'Enable debug logging'
        type: boolean
        default: false

permissions:
  contents: write  # For updating the quarantine file
  issues: write    # For creating issues for flaky tests
  pull-requests: write # For commenting on PRs
  actions: read    # For reading workflow runs

jobs:
  analyze-test-stability:
    name: Analyze Test Stability
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Get full history
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas matplotlib
      
      - name: Analyze workflow runs
        id: analysis
        run: |
          mkdir -p TestResults/Stability
          
          cat > analyze_stability.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import sys
          import json
          import requests
          import datetime
          import re
          import pandas as pd
          import matplotlib.pyplot as plt
          from collections import defaultdict
          
          # Configuration
          REPO = os.environ.get('GITHUB_REPOSITORY')
          TOKEN = os.environ.get('GITHUB_TOKEN')
          HEADERS = {
              'Authorization': f'token {TOKEN}',
              'Accept': 'application/vnd.github+json'
          }
          MAX_RUNS = 100
          INSTABILITY_THRESHOLD = 0.2  # 20% failure rate
          QUARANTINE_FILE = 'Tests/quarantine.json'
          DEBUG = os.environ.get('DEBUG_ENABLED', '').lower() == 'true'
          
          # Initialize data structures
          test_results = defaultdict(lambda: {'pass': 0, 'fail': 0, 'skip': 0})
          test_history = defaultdict(list)
          
          def log(message):
              print(message)
              sys.stdout.flush()
          
          def debug(message):
              if DEBUG:
                  print(f"DEBUG: {message}")
                  sys.stdout.flush()
          
          def get_workflow_runs():
              url = f'https://api.github.com/repos/{REPO}/actions/workflows/main.yml/runs?status=completed&per_page={MAX_RUNS}'
              response = requests.get(url, headers=HEADERS)
              if response.status_code != 200:
                  log(f"Error fetching workflow runs: {response.status_code}")
                  return []
              
              data = response.json()
              return data.get('workflow_runs', [])
          
          def get_test_results(run_id):
              # Get artifacts for the run
              artifacts_url = f'https://api.github.com/repos/{REPO}/actions/runs/{run_id}/artifacts'
              response = requests.get(artifacts_url, headers=HEADERS)
              if response.status_code != 200:
                  log(f"Error fetching artifacts for run {run_id}: {response.status_code}")
                  return {}
                  
              artifacts = response.json().get('artifacts', [])
              test_results = {}
              
              # Look for test result artifacts
              for artifact in artifacts:
                  if artifact['name'].startswith('test-results-'):
                      # Extract test type from artifact name
                      test_type = artifact['name'].split('-')[2]
                      
                      # For each test type, record results
                      test_results[test_type] = {
                          'total': 0,
                          'passed': 0,
                          'failed': 0,
                          'skipped': 0
                      }
                  
              # Also parse the run logs to get test results
              logs_url = f'https://api.github.com/repos/{REPO}/actions/runs/{run_id}/logs'
              try:
                  debug(f"Fetching logs for run {run_id}")
                  # For demo, we'll simulate parsing logs
                  for test_type in test_results:
                      # In a real implementation, you would parse the actual logs
                      # This is just a simulation for the example
                      test_results[test_type]['total'] = 20
                      test_results[test_type]['passed'] = 18
                      test_results[test_type]['failed'] = 1
                      test_results[test_type]['skipped'] = 1
              except Exception as e:
                  log(f"Error parsing logs for run {run_id}: {str(e)}")
              
              return test_results
          
          def load_quarantine_file():
              try:
                  if os.path.exists(QUARANTINE_FILE):
                      with open(QUARANTINE_FILE, 'r') as f:
                          return json.load(f)
                  return {}
              except Exception as e:
                  log(f"Error loading quarantine file: {str(e)}")
                  return {}
          
          def save_quarantine_file(data):
              try:
                  with open(QUARANTINE_FILE, 'w') as f:
                      json.dump(data, f, indent=2)
                  log(f"Updated quarantine file: {QUARANTINE_FILE}")
              except Exception as e:
                  log(f"Error saving quarantine file: {str(e)}")
          
          def analyze_test_stability():
              log("Analyzing test stability...")
              
              runs = get_workflow_runs()
              log(f"Found {len(runs)} workflow runs to analyze")
              
              for run in runs[:30]:  # Limit to 30 most recent runs to avoid rate limits
                  run_id = run['id']
                  run_date = run['created_at']
                  
                  debug(f"Analyzing run {run_id} from {run_date}")
                  results = get_test_results(run_id)
                  
                  for test_name, result in results.items():
                      test_results[test_name]['pass'] += result['passed']
                      test_results[test_name]['fail'] += result['failed']
                      test_results[test_name]['skip'] += result['skipped']
                      
                      # Record test history (pass/fail status)
                      test_history[test_name].append({
                          'date': run_date,
                          'status': 'pass' if result['failed'] == 0 else 'fail',
                          'run_id': run_id
                      })
              
              # Identify flaky tests
              quarantine_candidates = []
              for test_name, counts in test_results.items():
                  total = counts['pass'] + counts['fail']
                  if total > 0:
                      failure_rate = counts['fail'] / total
                      if failure_rate > 0 and failure_rate < INSTABILITY_THRESHOLD:
                          quarantine_candidates.append({
                              'name': test_name,
                              'failure_rate': failure_rate,
                              'passes': counts['pass'],
                              'failures': counts['fail'],
                              'skips': counts['skip']
                          })
              
              # Sort by failure rate
              quarantine_candidates.sort(key=lambda x: x['failure_rate'], reverse=True)
              
              log(f"Found {len(quarantine_candidates)} potential flaky tests")
              
              # Generate report
              with open('TestResults/Stability/stability_report.md', 'w') as f:
                  f.write("# Test Stability Analysis\n\n")
                  f.write(f"Analysis performed on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  
                  f.write("## Potentially Flaky Tests\n\n")
                  if quarantine_candidates:
                      f.write("| Test | Failure Rate | Passes | Failures | Skips |\n")
                      f.write("|------|-------------|--------|----------|-------|\n")
                      for test in quarantine_candidates:
                          f.write(f"| {test['name']} | {test['failure_rate']:.1%} | {test['passes']} | {test['failures']} | {test['skips']} |\n")
                  else:
                      f.write("No flaky tests detected!\n")
                  
                  f.write("\n## Overall Test Stability\n\n")
                  f.write("| Test | Success Rate | Failures | Total Runs |\n")
                  f.write("|------|-------------|----------|------------|\n")
                  
                  for test_name, counts in test_results.items():
                      total = counts['pass'] + counts['fail']
                      if total > 0:
                          success_rate = counts['pass'] / total
                          f.write(f"| {test_name} | {success_rate:.1%} | {counts['fail']} | {total} |\n")
              
              # Update quarantine file
              current_quarantine = load_quarantine_file()
              updated = False
              
              for candidate in quarantine_candidates:
                  test_name = candidate['name']
                  if test_name not in current_quarantine:
                      # Add new flaky test to quarantine
                      current_quarantine[test_name] = {
                          "reason": f"Automatically detected as flaky ({candidate['failure_rate']:.1%} failure rate)",
                          "skipInCI": True,
                          "failureRate": candidate['failure_rate'],
                          "created": datetime.datetime.now().isoformat()
                      }
                      updated = True
              
              if updated:
                  save_quarantine_file(current_quarantine)
                  log(f"Updated quarantine file with {len(quarantine_candidates)} flaky tests")
              
              return quarantine_candidates
          
          if __name__ == "__main__":
              flaky_tests = analyze_test_stability()
              
              # Output number of flaky tests for GitHub Actions
              print(f"::set-output name=flaky_test_count::{len(flaky_tests)}")
              
              # Generate summary metrics
              with open('TestResults/Stability/metrics.json', 'w') as f:
                  json.dump({
                      'flaky_tests': len(flaky_tests),
                      'analyzed_tests': len(test_results),
                      'quarantine_threshold': INSTABILITY_THRESHOLD
                  }, f)
              
              # Also try to visualize the data
              try:
                  # Create a simple bar chart of failure rates
                  data = []
                  labels = []
                  for test, counts in test_results.items():
                      total = counts['pass'] + counts['fail']
                      if total > 0:
                          failure_rate = counts['fail'] / total
                          data.append(failure_rate * 100)  # Convert to percentage
                          labels.append(test)
                  
                  if data:
                      plt.figure(figsize=(10, 6))
                      plt.bar(labels, data)
                      plt.axhline(y=INSTABILITY_THRESHOLD * 100, color='r', linestyle='-', label='Quarantine Threshold')
                      plt.xlabel('Tests')
                      plt.ylabel('Failure Rate (%)')
                      plt.title('Test Failure Rates')
                      plt.xticks(rotation=45, ha='right')
                      plt.tight_layout()
                      plt.savefig('TestResults/Stability/failure_rates.png')
                      log("Generated failure rate visualization")
              except Exception as e:
                  log(f"Error generating visualization: {str(e)}")
          EOF
          
          # Make script executable and run it
          chmod +x analyze_stability.py
          DEBUG_ENABLED=${{ inputs.debug_enabled }} GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }} python analyze_stability.py
          
          # Set outputs
          echo "flaky_test_count=$(jq -r '.flaky_tests' TestResults/Stability/metrics.json)" >> $GITHUB_OUTPUT
      
      - name: Create GitHub issues for flaky tests
        if: steps.analysis.outputs.flaky_test_count > 0
        run: |
          # Parse the stability report to create issues
          python - << 'EOF'
          import os
          import re
          import json
          import requests
          
          REPO = os.environ.get('GITHUB_REPOSITORY')
          TOKEN = os.environ.get('GITHUB_TOKEN')
          HEADERS = {
              'Authorization': f'token {TOKEN}',
              'Accept': 'application/vnd.github+json'
          }
          
          with open('TestResults/Stability/stability_report.md', 'r') as f:
              report = f.read()
          
          # Extract table of flaky tests
          table_match = re.search(r'## Potentially Flaky Tests\s+\n(.*?)(?=\n\n)', report, re.DOTALL)
          if table_match:
              table = table_match.group(1)
              
              # Parse table rows
              test_pattern = r'\| (.*?) \| (.*?) \| (\d+) \| (\d+) \| (\d+) \|'
              tests = re.findall(test_pattern, table)
              
              for test in tests:
                  test_name, failure_rate, passes, failures, skips = test
                  
                  # Check if issue already exists
                  search_url = f'https://api.github.com/search/issues?q=repo:{REPO}+is:issue+is:open+in:title+"Flaky test: {test_name}"'
                  response = requests.get(search_url, headers=HEADERS)
                  
                  if response.status_code == 200 and response.json()['total_count'] == 0:
                      # Create new issue
                      issue_body = f"""
          ## Flaky Test Detected
          
          The automated test stability analysis has detected a potentially flaky test:
          
          - **Test**: `{test_name}`
          - **Failure Rate**: {failure_rate}
          - **Passes**: {passes}
          - **Failures**: {failures}
          - **Skips**: {skips}
          
          ### Next Steps
          
          1. This test has been automatically added to the quarantine list
          2. Investigate the causes of flakiness
          3. Fix the underlying issue
          4. Remove from quarantine once fixed
          
          The test will continue to run in local environments but will be skipped in CI until fixed.
          """
                      
                      issue_data = {
                          'title': f"Flaky test: {test_name}",
                          'body': issue_body,
                          'labels': ['bug', 'flaky-test', 'test-infrastructure']
                      }
                      
                      create_url = f'https://api.github.com/repos/{REPO}/issues'
                      response = requests.post(create_url, headers=HEADERS, json=issue_data)
                      
                      if response.status_code == 201:
                          print(f"Created issue for flaky test: {test_name}")
                      else:
                          print(f"Failed to create issue for {test_name}: {response.status_code}")
          EOF
      
      - name: Commit quarantine updates
        id: commit
        run: |
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions"
          
          # Check if changes were made to the quarantine file
          if git diff --name-only | grep -q "Tests/quarantine.json"; then
            git add Tests/quarantine.json
            git commit -m "Update test quarantine [skip ci]"
            git push
            echo "changes_made=true" >> $GITHUB_OUTPUT
          else
            echo "No changes to quarantine file"
            echo "changes_made=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload stability report
        uses: actions/upload-artifact@v4
        with:
          name: test-stability-report
          path: TestResults/Stability
          retention-days: 90
      
      - name: Create PR summary
        run: |
          cat TestResults/Stability/stability_report.md > $GITHUB_STEP_SUMMARY
      
      - name: Notify team of quarantine updates
        if: steps.commit.outputs.changes_made == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            // Create a notification issue or discussion
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸ”„ Test Quarantine Updated',
              body: `## Test Quarantine Update

            The test quarantine file has been automatically updated based on stability analysis.

            [View the full stability report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})

            ### New Quarantined Tests

            Please review the issues created for each flaky test and prioritize fixes.

            ### Next Steps

            1. Investigate the root causes of flakiness
            2. Fix the underlying issues
            3. Remove tests from quarantine once fixed`
            }); 